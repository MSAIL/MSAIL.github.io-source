Title: Intro to Statistical Learning Theory
Date: January 26, 2016 7:00 PM
Location: DOW 2166
Category: Reading Group
Summary:  Is learning possible?  Yes...  in some cases.  This week we'll investigate when and why machines are able to generalize, viewing the standard machine learning algorithms we know and love under the lens of statistical learning theory, which allows us to understand the strengths and limitations of the algorithms we use every day.  We'll cover important theoretical concepts like VC dimension and PAC learning.

### Core Paper

- Kulkarni & Harman 2011, ["Statistical Learning Theory:  A Tutorial"](http://www.princeton.edu/~harman/Papers/SLT-tutorial.pdf)

### Additional Resources

- **Paper**:  Anthony 1993, ["A Result of Vapnik with Applications"](http://www.sciencedirect.com/science/article/pii/0166218X93901269)
- **Slides**:  ["MLSS:  Statistical Learning Theory"](http://www-stat.wharton.upenn.edu/~rakhlin/ml_summer_school.pdf)
- **Lectures**:  ["Caltech:  Learning from Data"](http://work.caltech.edu/lectures.html#lectures)

### Summary

Is learning possible?  Yes...  in some cases.  This week we'll investigate when and why machines are able to generalize, viewing the standard machine learning algorithms we know and love under the lens of statistical learning theory, which allows us to understand the strengths and limitations of the algorithms we use every day.  We'll cover important theoretical concepts like VC dimension and PAC learning.

This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.